{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "mount_file_id": "1lSyCEkB0-DM0WRomOK3Ih2lldTFhsmag",
      "authorship_tag": "ABX9TyNN2VbbMBajXzo794ip34CE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atlimited/llm_train/blob/main/deepseek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXywgHLhQ-I8",
        "outputId": "258d8b2e-f8a9-4689-a93d-8e3d5e82f200"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/train.txt ."
      ],
      "metadata": {
        "id": "k___LU02wh_3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/deepseek-ai/deepseek-moe-16b-base"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbvBs_zeMUVr",
        "outputId": "f0aac231-37de-4ca3-a71b-0c702e74c6e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deepseek-moe-16b-base'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 40 (delta 12), reused 0 (delta 0), pack-reused 4\u001b[K\n",
            "Unpacking objects: 100% (40/40), 1.38 MiB | 3.92 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir deepseek\n",
        "!cp deepseek-moe-16b-base/*.json deepseek/\n",
        "!cp deepseek-moe-16b-base/*.py deepseek/"
      ],
      "metadata": {
        "id": "giy6UD-JMsis"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh deepseek"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX5UFOw-OWJM",
        "outputId": "9b0fa93a-994f-4634-cfd5-e7010e1a7b3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 5.0M\n",
            "-rw-r--r-- 1 root root 1.1K Apr 22 05:24 config.json\n",
            "-rw-r--r-- 1 root root  10K Apr 22 05:24 configuration_deepseek.py\n",
            "-rw-r--r-- 1 root root  121 Apr 22 05:24 generation_config.json\n",
            "-rw-r--r-- 1 root root  72K Apr 22 05:24 modeling_deepseek.py\n",
            "-rw-r--r-- 1 root root 479K Apr 22 05:24 model.safetensors.index.json\n",
            "-rw-r--r-- 1 root root  793 Apr 22 05:24 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 4.4M Apr 22 05:24 tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/transformers.git -b v4.38.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvlyt6sLw18r",
        "outputId": "b656aa51-6f02-4dc5-9b89-41df46222fc0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 196791, done.\u001b[K\n",
            "remote: Counting objects: 100% (1915/1915), done.\u001b[K\n",
            "remote: Compressing objects: 100% (951/951), done.\u001b[K\n",
            "remote: Total 196791 (delta 1187), reused 1399 (delta 841), pack-reused 194876\u001b[K\n",
            "Receiving objects: 100% (196791/196791), 210.18 MiB | 14.41 MiB/s, done.\n",
            "Resolving deltas: 100% (139410/139410), done.\n",
            "Note: switching to '092f1fdaa4224fdd88c616dc9678e6fcb37bfffd'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp transformers/examples/pytorch/language-modeling/run_clm.py ."
      ],
      "metadata": {
        "id": "2pA6PPvryZvH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "an_bQYv9Pd6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiqB_rO1Uoc-",
        "outputId": "cbe4f915-498a-4ff0-c86e-cd0b2b8530c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deepseek  deepseek-moe-16b-base  drive\trun_clm.py  sample_data  train.txt  transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r transformers/examples/pytorch/language-modeling/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epPj8jx9YVo5",
        "outputId": "871c6822-4f44-4aa8-a0a7-84fa88fa8b19"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate>=0.12.0 (from -r transformers/examples/pytorch/language-modeling/requirements.txt (line 1))\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/297.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from -r transformers/examples/pytorch/language-modeling/requirements.txt (line 2)) (2.2.1+cu121)\n",
            "Collecting datasets>=1.8.0 (from -r transformers/examples/pytorch/language-modeling/requirements.txt (line 3))\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/542.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.10/dist-packages (from -r transformers/examples/pytorch/language-modeling/requirements.txt (line 4)) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r transformers/examples/pytorch/language-modeling/requirements.txt (line 5)) (3.20.3)\n",
            "Collecting evaluate (from -r transformers/examples/pytorch/language-modeling/requirements.txt (line 6))\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r transformers/examples/pytorch/language-modeling/requirements.txt (line 7)) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 1)) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 1)) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 1)) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2)) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2)) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2))\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2)) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2))\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (4.66.2)\n",
            "Collecting xxhash (from datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3))\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (3.9.5)\n",
            "Collecting huggingface-hub (from accelerate>=0.12.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from evaluate->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 6))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 7)) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 7)) (3.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.8.0->-r transformers/examples/pytorch/language-modeling/requirements.txt (line 3)) (1.16.0)\n",
            "Installing collected packages: xxhash, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, responses, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, nvidia-cusolver-cu12, datasets, evaluate, accelerate\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed accelerate-0.29.3 datasets-2.19.0 dill-0.3.8 evaluate-0.4.1 huggingface-hub-0.22.2 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 responses-0.18.0 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9BEyEhJYVvH",
        "outputId": "bdd74a84-94f8-4183-ca22-d8a7a0a8d1c7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.5.7.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.2.1+cu121)\n",
            "Collecting einops (from flash-attn)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (24.0)\n",
            "Collecting ninja (from flash-attn)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.5.7-cp310-cp310-linux_x86_64.whl size=120853563 sha256=bbe6f77fd0899f8a125a5bdcf734b660c4c88e81c9b51c7ce98ebeba44dc6fa0\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/96/ed/bcac89c56b606421f99b45b16a94db5d0f2b6b4eaf8bac4d01\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: ninja, einops, flash-attn\n",
            "Successfully installed einops-0.7.0 flash-attn-2.5.7 ninja-1.11.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/deepseek/config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7W4deNYCSzc",
        "outputId": "fa4f73da-9728-48f2-954d-77bc5a839700"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"architectures\": [\n",
            "    \"DeepseekForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"configuration_deepseek.DeepseekConfig\",\n",
            "    \"AutoModel\": \"modeling_deepseek.DeepseekModel\",\n",
            "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 100000,\n",
            "  \"eos_token_id\": 100001,\n",
            "  \"first_k_dense_replace\": 1,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 10944,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"deepseek\",\n",
            "  \"moe_intermediate_size\": 1408,\n",
            "  \"moe_layer_freq\": 1,\n",
            "  \"n_routed_experts\": 64,\n",
            "  \"n_shared_experts\": 2,\n",
            "  \"norm_topk_prob\": false,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_experts_per_tok\": 6,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 16,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000,\n",
            "  \"scoring_func\": \"softmax\",\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.36.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 102400\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/deepseek/config.json\n",
        "\n",
        "{\n",
        "  \"architectures\": [\n",
        "    \"DeepseekForCausalLM\"\n",
        "  ],\n",
        "  \"attention_bias\": false,\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"auto_map\": {\n",
        "    \"AutoConfig\": \"configuration_deepseek.DeepseekConfig\",\n",
        "    \"AutoModel\": \"modeling_deepseek.DeepseekModel\",\n",
        "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekForCausalLM\"\n",
        "  },\n",
        "  \"bos_token_id\": 100000,\n",
        "  \"eos_token_id\": 100001,\n",
        "  \"first_k_dense_replace\": 1,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 512,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 10944,\n",
        "  \"max_position_embeddings\": 4096,\n",
        "  \"model_type\": \"deepseek\",\n",
        "  \"moe_intermediate_size\": 1408,\n",
        "  \"moe_layer_freq\": 1,\n",
        "  \"n_routed_experts\": 8,\n",
        "  \"n_shared_experts\": 2,\n",
        "  \"norm_topk_prob\": false,\n",
        "  \"num_attention_heads\": 16,\n",
        "  \"num_experts_per_tok\": 6,\n",
        "  \"num_hidden_layers\": 14,\n",
        "  \"num_key_value_heads\": 16,\n",
        "  \"pretraining_tp\": 1,\n",
        "  \"rms_norm_eps\": 1e-06,\n",
        "  \"rope_scaling\": null,\n",
        "  \"rope_theta\": 10000,\n",
        "  \"scoring_func\": \"softmax\",\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.36.0\",\n",
        "  \"use_cache\": true,\n",
        "  \"vocab_size\": 102400\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ONdNbUg435T",
        "outputId": "48832e90-27fc-4bbd-ae66-2e217d92748e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/deepseek/config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from deepseek.modeling_deepseek import DeepseekForCausalLM\n",
        "from deepseek.configuration_deepseek import DeepseekConfig\n",
        "\n",
        "model_dir = 'deepseek'\n",
        "config_file = './deepseek/config.json'\n",
        "with open(config_file, 'r') as f:\n",
        "    config = json.load(f)\n",
        "    config = DeepseekConfig.from_dict(config)\n",
        "\n",
        "model = DeepseekForCausalLM(config)\n",
        "\n",
        "model.save_pretrained(model_dir)"
      ],
      "metadata": {
        "id": "wlxl57T-0oLM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp train.txt ori.txt\n",
        "!head -n 100 ori.txt > train.txt\n",
        "!tail -n 100 ori.txt > val.txt"
      ],
      "metadata": {
        "id": "Wo6ksxI5n2dX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clm.py \\\n",
        "    --model_name_or_path=./deepseek \\\n",
        "    --train_file=train.txt \\\n",
        "    --validation_file=val.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --num_train_epochs=30 \\\n",
        "    --save_steps=10000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --per_device_train_batch_size=1 \\\n",
        "    --per_device_eval_batch_size=1 \\\n",
        "    --output_dir=output/ \\\n",
        "    --overwrite_output_dir=true \\\n",
        "    --use_fast_tokenizer=False \\\n",
        "    --trust_remote_code=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORvTha_jyoy7",
        "outputId": "51337689-c5b9-4f5d-db53-4e49c60bbed2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-22 05:25:58.534181: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-22 05:25:58.534240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-22 05:25:58.535553: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-22 05:25:59.654613: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/22/2024 05:26:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/22/2024 05:26:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/runs/Apr22_05-26-03_9556c8d2f8ec,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=output/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=1,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output/,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=10000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-9a6ee1977c9410f0\n",
            "04/22/2024 05:26:04 - INFO - datasets.builder - Using custom data configuration default-9a6ee1977c9410f0\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n",
            "04/22/2024 05:26:04 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text\n",
            "Generating dataset text (/root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101)\n",
            "04/22/2024 05:26:04 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101)\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101...\n",
            "04/22/2024 05:26:04 - INFO - datasets.builder - Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101...\n",
            "Downloading took 0.0 min\n",
            "04/22/2024 05:26:04 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "04/22/2024 05:26:04 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "04/22/2024 05:26:04 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 100 examples [00:00, 7728.87 examples/s]\n",
            "Generating validation split\n",
            "04/22/2024 05:26:04 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 100 examples [00:00, 94127.11 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "04/22/2024 05:26:04 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101. Subsequent calls will reuse this data.\n",
            "04/22/2024 05:26:04 - INFO - datasets.builder - Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:726] 2024-04-22 05:26:04,345 >> loading configuration file ./deepseek/config.json\n",
            "[INFO|configuration_utils.py:726] 2024-04-22 05:26:04,353 >> loading configuration file ./deepseek/config.json\n",
            "[INFO|configuration_utils.py:791] 2024-04-22 05:26:04,354 >> Model config DeepseekConfig {\n",
            "  \"_name_or_path\": \"./deepseek\",\n",
            "  \"architectures\": [\n",
            "    \"DeepseekForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"configuration_deepseek.DeepseekConfig\",\n",
            "    \"AutoModel\": \"modeling_deepseek.DeepseekModel\",\n",
            "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekForCausalLM\"\n",
            "  },\n",
            "  \"aux_loss_alpha\": 0.001,\n",
            "  \"bos_token_id\": 100000,\n",
            "  \"eos_token_id\": 100001,\n",
            "  \"first_k_dense_replace\": 1,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 10944,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"deepseek\",\n",
            "  \"moe_intermediate_size\": 1408,\n",
            "  \"moe_layer_freq\": 1,\n",
            "  \"n_routed_experts\": 8,\n",
            "  \"n_shared_experts\": 2,\n",
            "  \"norm_topk_prob\": false,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_experts_per_tok\": 6,\n",
            "  \"num_hidden_layers\": 14,\n",
            "  \"num_key_value_heads\": 16,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000,\n",
            "  \"scoring_func\": \"softmax\",\n",
            "  \"seq_aux\": true,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 102400\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2044] 2024-04-22 05:26:04,359 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2044] 2024-04-22 05:26:04,359 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2044] 2024-04-22 05:26:04,359 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2044] 2024-04-22 05:26:04,359 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2044] 2024-04-22 05:26:04,359 >> loading file tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-04-22 05:26:04,596 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|modeling_utils.py:3254] 2024-04-22 05:26:04,626 >> loading weights file ./deepseek/model.safetensors\n",
            "[INFO|configuration_utils.py:845] 2024-04-22 05:26:04,649 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 100000,\n",
            "  \"eos_token_id\": 100001\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3992] 2024-04-22 05:26:05,128 >> All model checkpoint weights were used when initializing DeepseekForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4000] 2024-04-22 05:26:05,128 >> All the weights of DeepseekForCausalLM were initialized from the model checkpoint at ./deepseek.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DeepseekForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:798] 2024-04-22 05:26:05,133 >> loading configuration file ./deepseek/generation_config.json\n",
            "[INFO|configuration_utils.py:845] 2024-04-22 05:26:05,133 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 100000,\n",
            "  \"eos_token_id\": 100001\n",
            "}\n",
            "\n",
            "Running tokenizer on dataset:   0% 0/100 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101/cache-689c75842e3b11d3.arrow\n",
            "04/22/2024 05:26:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101/cache-689c75842e3b11d3.arrow\n",
            "Running tokenizer on dataset: 100% 100/100 [00:00<00:00, 5093.51 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/100 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101/cache-d6fc8978b6a1fb09.arrow\n",
            "04/22/2024 05:26:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101/cache-d6fc8978b6a1fb09.arrow\n",
            "Running tokenizer on dataset: 100% 100/100 [00:00<00:00, 8134.01 examples/s]\n",
            "04/22/2024 05:26:05 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (16384). Using block_size=1024 instead. You can change that default value by passing --block_size xxx.\n",
            "Grouping texts in chunks of 1024:   0% 0/100 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101/cache-3404adb60e9b12ed.arrow\n",
            "04/22/2024 05:26:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101/cache-3404adb60e9b12ed.arrow\n",
            "Grouping texts in chunks of 1024: 100% 100/100 [00:00<00:00, 7111.88 examples/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/100 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101/cache-ce246b811abe84eb.arrow\n",
            "04/22/2024 05:26:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-9a6ee1977c9410f0/0.0.0/96636a050ef51804b84abbfd4f4ad440e01153c24b86293eb5c3b300a41f9101/cache-ce246b811abe84eb.arrow\n",
            "Grouping texts in chunks of 1024: 100% 100/100 [00:00<00:00, 7143.13 examples/s]\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 16.4MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1812] 2024-04-22 05:26:07,842 >> ***** Running training *****\n",
            "[INFO|trainer.py:1813] 2024-04-22 05:26:07,843 >>   Num examples = 7\n",
            "[INFO|trainer.py:1814] 2024-04-22 05:26:07,843 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1815] 2024-04-22 05:26:07,843 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:1818] 2024-04-22 05:26:07,843 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1819] 2024-04-22 05:26:07,843 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1820] 2024-04-22 05:26:07,843 >>   Total optimization steps = 210\n",
            "[INFO|trainer.py:1821] 2024-04-22 05:26:07,844 >>   Number of trainable parameters = 417,565,184\n",
            "100% 210/210 [02:57<00:00,  1.17it/s][INFO|trainer.py:2067] 2024-04-22 05:29:04,876 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 177.1496, 'train_samples_per_second': 1.185, 'train_steps_per_second': 1.185, 'train_loss': 6.256693522135417, 'epoch': 30.0}\n",
            "100% 210/210 [02:57<00:00,  1.19it/s]\n",
            "[INFO|trainer.py:3067] 2024-04-22 05:29:04,996 >> Saving model checkpoint to output/\n",
            "[INFO|configuration_utils.py:473] 2024-04-22 05:29:05,000 >> Configuration saved in output/config.json\n",
            "[INFO|configuration_utils.py:614] 2024-04-22 05:29:05,000 >> Configuration saved in output/generation_config.json\n",
            "[INFO|modeling_utils.py:2454] 2024-04-22 05:29:08,888 >> Model weights saved in output/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2459] 2024-04-22 05:29:08,889 >> tokenizer config file saved in output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2468] 2024-04-22 05:29:08,889 >> Special tokens file saved in output/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     6.2567\n",
            "  train_runtime            = 0:02:57.14\n",
            "  train_samples            =          7\n",
            "  train_samples_per_second =      1.185\n",
            "  train_steps_per_second   =      1.185\n",
            "04/22/2024 05:29:08 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:3376] 2024-04-22 05:29:08,954 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3378] 2024-04-22 05:29:08,954 >>   Num examples = 8\n",
            "[INFO|trainer.py:3381] 2024-04-22 05:29:08,954 >>   Batch size = 1\n",
            "100% 8/8 [00:01<00:00,  4.99it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.0947\n",
            "  eval_loss               =     9.5561\n",
            "  eval_runtime            = 0:00:02.01\n",
            "  eval_samples            =          8\n",
            "  eval_samples_per_second =      3.969\n",
            "  eval_steps_per_second   =      3.969\n",
            "  perplexity              = 14130.4524\n",
            "[INFO|modelcard.py:450] 2024-04-22 05:29:10,973 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.0946969696969697}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FkPlZVrOn2iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f6IC_L8gn2ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LL9cFuvOn2oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W7S7bNDHn2qh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}